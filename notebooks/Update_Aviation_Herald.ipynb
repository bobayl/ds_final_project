{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q0'></a>\n",
    "<center> <h1> Aviation Herald Project: Update the Dataset</h1> </center>\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<center><h4>Laurent Bobay, 2024</h4></center>\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "<div style=\"background:#EEEDF5;border-top:0.1cm solid #EF475B;border-bottom:0.1cm solid #EF475B;\">\n",
    "    <div style=\"margin-left: 0.5cm;margin-top: 0.5cm;margin-bottom: 0.5cm;color:#303030\">\n",
    "        <p><strong>Goal:</strong> Create dataset of all publicly available articles and comments from www.avherald.com</p>\n",
    "        <strong> Outline:</strong>\n",
    "        <a id='P0' name=\"P0\"></a>\n",
    "        <ol>\n",
    "            <li> <a style=\"color:#303030\" href='#SU'>Set up</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P1'>Data Exploration and Cleaning</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P2'>Modeling</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P3'>Model Evaluation</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#CL'>Conclusion</a></li>\n",
    "        </ol>\n",
    "        <strong>Topics Trained:</strong> Notebook Layout, Data Cleaning, Modelling and Model Evaluation\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<nav style=\"text-align:right\"><strong>\n",
    "        <a style=\"color:#00BAE5\" href=\"https://monolith.propulsion-home.ch/backend/api/momentum/materials/ds-materials/07_MLEngineering/index.html\" title=\"momentum\"> Module 7, Machine Learning Engineering </a>|\n",
    "        <a style=\"color:#00BAE5\" href=\"https://monolith.propulsion-home.ch/backend/api/momentum/materials/ds-materials/07_MLEngineering/day1/index.html\" title=\"momentum\">Day 1, Data Science Project Development </a>|\n",
    "        <a style=\"color:#00BAE5\" href=\"https://drive.google.com/file/d/1SOCQu9Gv3jNNXxvJSszBC3fYNsM0df2F/view?usp=sharing\" title=\"momentum\"> Live Coding 1, Simple Prediction Notebook</a>\n",
    "</strong></nav>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scraping_helpers import *\n",
    "from preprocessing_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File path for the local storeage\n",
    "# filepath = \"../data/processed/av_dataset.csv\"\n",
    "# df = pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "\n",
    "# df0 = df.iloc[:6000]\n",
    "# df1 = df.iloc[6000:18000]\n",
    "# df2 = df.iloc[18000:]\n",
    "\n",
    "# # Write them to file\n",
    "# filepath0 = \"../data/processed/av_dataset0.csv\"\n",
    "# filepath1 = \"../data/processed/av_dataset1.csv\"\n",
    "# filepath2 = \"../data/processed/av_dataset2.csv\"\n",
    "# df0.to_csv(filepath0, sep=',', index=False, header=True, na_rep='NULL', encoding='utf-8')\n",
    "# df1.to_csv(filepath1, sep=',', index=False, header=True, na_rep='NULL', encoding='utf-8')\n",
    "# df2.to_csv(filepath2, sep=',', index=False, header=True, na_rep='NULL', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(filepath):\n",
    "    \"\"\" The function takes the filepath to the full dataset and then updates that dataset by scraping the new entries on the webiste\"\"\"\n",
    "\n",
    "    # The dataset\n",
    "    filepath = \"../data/processed/av_dataset.csv\"\n",
    "    \n",
    "    # Read in the current (non-up-to-date) datset\n",
    "    old_df = pd.read_csv(filepath)\n",
    "    print(f\"Length of previous df: {len(old_df)}\")\n",
    "\n",
    "    # From the dataset read the href of the first row\n",
    "    last_href = old_df.iloc[0].href\n",
    "\n",
    "\n",
    "    \"\"\" Scrape missing items \"\"\"\n",
    "\n",
    "    # Entry\n",
    "    URL = \"https://avherald.com\"\n",
    "\n",
    "    # Scrape hrefs and titles\n",
    "    titles, hrefs = tqdm(get_new_titles_and_hrefs(URL, last_href), desc=\"Loading hrefs\") # titles is a dict {title: href}\n",
    "\n",
    "    # Scrape titles, articles, comments unitl that href\n",
    "    texts = []\n",
    "    time_authors = []\n",
    "    headlines = []\n",
    "    comment_authors = []\n",
    "    comments = []\n",
    "    occurrences = []\n",
    "    urls = []\n",
    "\n",
    "    for href in tqdm(hrefs, desc=\"Updating Dataset\"):\n",
    "        url = URL + href\n",
    "        page = load_page(url)\n",
    "        article_text, time_author = get_article(page)\n",
    "        headline_text, comment_authors_texts, comments_texts = get_comments(page)\n",
    "        occurrence = find_occurrence_type(headline_text)\n",
    "\n",
    "        texts.append(article_text)\n",
    "        time_authors.append(time_author)\n",
    "        headlines.append(headline_text)\n",
    "        comment_authors.append(comment_authors_texts)\n",
    "        comments.append(comments_texts)\n",
    "        occurrences.append(occurrence)\n",
    "        urls.append(url)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"title\": titles,\n",
    "        \"href\": hrefs,\n",
    "        \"text\": texts,\n",
    "        \"time_author\": time_authors,\n",
    "        \"headline\": headlines,\n",
    "        \"comment_authors\": comment_authors,\n",
    "        \"comments\": comments,\n",
    "        \"occurrence\": occurrences,\n",
    "        \"url\": urls\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\" If there are no updates, return the old_df\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"Nothing to update\")\n",
    "        return old_df\n",
    "    else:\n",
    "        # Print the number of new articles\n",
    "        print(f\"{len(df)} new articles found\")\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Preprocess the update_df\"\"\"\n",
    "\n",
    "    # Apply the function to each value in 'Input' column\n",
    "    df[\"author\"], df[\"created\"], df[\"updated\"] = zip(*df[\"time_author\"].apply(get_author_and_time))\n",
    "\n",
    "    # Make sure all items in 'text' column are string\n",
    "    df['text'] = df['text'].apply(lambda x: str(x) if pd.notna(x) else \" \")\n",
    "\n",
    "    # Remove all linebrakes, tabs, etc. in the texts\n",
    "    def remove_linebreaks(text):\n",
    "        return re.sub(r'[\\n\\r\\t\\s]+', ' ', text, flags=re.UNICODE)\n",
    "    df[\"text\"] = df[\"text\"].apply(remove_linebreaks)\n",
    "\n",
    "    # Ensure necessary NLTK resources are downloaded\n",
    "    nltk.download('punkt');\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Initialize geonamescache\n",
    "    gc = geonamescache.GeonamesCache()\n",
    "\n",
    "    # Get a dictionary of cities and countries\n",
    "    cities = gc.get_cities()\n",
    "    countries = gc.get_countries()\n",
    "\n",
    "    # Extract city and country names\n",
    "    city_names = [remove_accented_chars(city['name']).lower() for city in cities.values()]\n",
    "    country_names = [remove_accented_chars(country['name']).lower() for country in countries.values()]\n",
    "\n",
    "    # Apply the preprocess_text function to each row in df[\"text\"] with tqdm progress bar\n",
    "    tqdm.pandas(desc=\"Normalizing texts\")  # This line enables tqdm support for Pandas apply function\n",
    "    df[\"normalized_text\"], df[\"cities\"], df[\"countries\"] = zip(*df[\"text\"].progress_apply(lambda x: preprocess_text(x, stop_words, city_names, country_names)))\n",
    "\n",
    "    # Drop rows where there is no text\n",
    "    df = df[df[\"text\"].notna()]\n",
    "\n",
    "    # Assign the flight phase to each new row\n",
    "    df[\"flight_phase\"] = df[\"text\"].apply(assign_flight_phase)\n",
    "\n",
    "    # Concatenate the update_df and the old_df:\n",
    "    df_new = pd.concat([df, old_df], ignore_index=True)\n",
    "\n",
    "    # Write the DataFrame to a CSV file with additional options\n",
    "    df_new.to_csv(filepath, sep=',', index=False, header=True, na_rep='NULL', encoding='utf-8')\n",
    "\n",
    "    \"\"\" Done with updating \"\"\"\n",
    "    print(\"Update complete\")\n",
    "\n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of previous df: 29025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading hrefs: 100%|██████████| 2/2 [00:00<00:00, 26296.58it/s]\n",
      "Updating Dataset: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "[nltk_data] Downloading package punkt to /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 new articles found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing texts: 100%|██████████| 3/3 [00:00<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update complete\n"
     ]
    }
   ],
   "source": [
    "df = update(filepath)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-top:0.1cm solid #EF475B\"></div>\n",
    "    <strong><a href='#Q0'><div style=\"text-align: right\"> <h3>End of this Notebook.</h3></div></a></strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
