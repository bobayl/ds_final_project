{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l-B3avlyc8c"
   },
   "source": [
    "# Text Pre-processing and Wrangling\n",
    "\n",
    "Text is different than usual datasets we use to build our typical Machine Learning models. Text data needs to be pre-processed to ensure we have it in a form that is usable for various NLP tasks. Text processing and wrangling is a necessary and important step in any NLP project.\n",
    "\n",
    "In this notebook, we will cover:\n",
    "- Sentence tokenization\n",
    "- Word tokenization\n",
    "- Handling non-text characters - accents, special symbols, HTML\n",
    "- Preprocessing \\ normalization steps such as stemming, lemmatization, expanding contractions and stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIaP2ot7zBGs"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "YEHEVTydzBsu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fppalTkNGQOW",
    "outputId": "a28fe136-040d-4201-e262-8c429b05f20c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /Users/laurent/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('europarl_raw')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgkdLpoezF0M"
   },
   "source": [
    "## Get Text Dataset\n",
    "\n",
    "Project Gutenberg is a large opensource and free collection of literary works from across the world. In this case, we will leverage content of the book **The Bible - Book 1: Genesis** for understanding different text processing and wrangling steps in the following sections\n",
    "\n",
    "\n",
    "We will also use a smaller sample text with a few short sentences to demostrate examples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RGdDEGiQzGQ3"
   },
   "outputs": [],
   "source": [
    "bible_html_url = \"http://www.gutenberg.org/cache/epub/8001/pg8001.html\"\n",
    "bible_txt_url = \"http://www.gutenberg.org/cache/epub/8001/pg8001.txt\"\n",
    "data = requests.get(bible_txt_url)\n",
    "content = data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqG5n0kWHpkV",
    "outputId": "38af48da-71bd-4d31-af92-70fe657fa19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in November 2002.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Book 01        Genesis\n",
      "\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "           Night. And the evening and the morning were the first day.\n",
      "\n",
      "01:001:006 And God said, Let there be a firmament in the midst of the\n",
      "           waters, and let it divide the waters from the waters.\n",
      "\n",
      "01:001:007 And God made the firmament, and divided the waters \n"
     ]
    }
   ],
   "source": [
    "print(content[970:1800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McHaUzY-Ga3Y",
    "outputId": "eee6b340-efdc-420b-a5ad-cffc58a7b034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total characters in Bible\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "pyQGCLZfG7l1",
    "outputId": "f63ad3b5-ea3a-4a86-805f-ad5cfbbd95c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \"\n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FMdH0H78GavY",
    "outputId": "3b549771-e04d-4225-92ce-4a5711266beb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis\\r\\n    \\r\\nThis ebook is '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "content[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OiKe5_Ry29q"
   },
   "source": [
    "## Sentence Tokenization\n",
    "\n",
    "Sentence is a syntactical as well as a logical division of content in a given corpus. In order to understand text or prepare it for various tasks, understanding sentence boundaries is an important step.\n",
    "\n",
    "Sentence tokenization is the process of determining sentence boundaries in a given corpus.\n",
    "\n",
    "One might think that it is merely trivial to determine a sentence boundary, we just need to split based on \".\". While this generally holds true yet there are exceptions. Think of scenarios where we use \".\" in abbreviations and shorthand notations(such as Mr. or Mrs.).\n",
    "\n",
    "\n",
    "Let us now go through a few standard ways of performing sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJDQdvp2y6Ar"
   },
   "source": [
    "### NLTK's Default Tokenizer\n",
    "\n",
    "NLTK provides a number of sentence tokenizers. Let's first have a look at the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXaTV42jy6ac",
    "outputId": "4a7139da-a4c5-4c33-c53b-fdf65f2b5e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text:4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "\n",
      "Total sentences in bible: 1589\n",
      "First 5 sentences in bible:-\n",
      "['\\ufeffThe Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis\\r\\n    \\r\\nThis ebook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.'\n",
      " 'You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this ebook or online\\r\\nat www.gutenberg.org.'\n",
      " 'If you are not located in the United States,\\r\\nyou will have to check the laws of the country where you are located\\r\\nbefore using this eBook.'\n",
      " \"Title: The Bible, King James version, Book 1: Genesis\\r\\n\\r\\nAuthor: Anonymous\\r\\n\\r\\nRelease date: April 1, 2005 [eBook #8001]\\r\\n                Most recently updated: December 26, 2020\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK THE BIBLE, KING JAMES VERSION, BOOK 1: GENESIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThis eBook was produced by David Widger\\r\\nwith the help of Derek Andrew's text from January 1992\\r\\nand the work of Bryan Taylor in November 2002.\"\n",
      " 'Book 01        Genesis\\r\\n\\r\\n01:001:001 In the beginning God created the heaven and the earth.']\n"
     ]
    }
   ],
   "source": [
    "bib_sentences = nltk.sent_tokenize(text=content)\n",
    "sample_sentences = nltk.sent_tokenize(text=sample_text)\n",
    "\n",
    "print('Total sentences in sample_text:{}'.format(len(sample_sentences)))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))\n",
    "\n",
    "print('\\nTotal sentences in bible:', len(bib_sentences))\n",
    "print('First 5 sentences in bible:-')\n",
    "print(np.array(bib_sentences[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "akq-AY_0Z9UR",
    "outputId": "77702b63-adfd-405d-c835-8321a226afbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5egAQjpewC9",
    "outputId": "7a11e99b-579d-4e4a-9e08-bc5d04fc3343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5CkXCmoy8p2"
   },
   "source": [
    "### Tokenize German Sentences\n",
    "\n",
    "NLTK also provides utilities to handle sentence tokenization for various languages (apart from English). The following is a sample to showcase sentence tokenization for German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IP3Ruqpty8-0",
    "outputId": "0e662def-43c9-42e0-d2d6-e5ad37a08b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total characters in the corpus\n",
    "print(len(german_text))\n",
    "# First 100 characters in the corpus\n",
    "print(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QxEr07ZYIDiv"
   },
   "outputs": [],
   "source": [
    "# default sentence tokenizer\n",
    "default_st = nltk.sent_tokenize\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRTKACJSfMbW",
    "outputId": "9b550b28-ef65-4b36-c5f8-af7d07431ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HatWlFiCIDaC",
    "outputId": "2c017c08-f301-4a2c-ae29-957121304c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if results of both tokenizers match\n",
    "# should be True\n",
    "print(german_sentences_def == german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmqmLhjZIP7R",
    "outputId": "ba9a20e5-19f8-4d39-8b2d-5dd2b6c332d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
      " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
      " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
      " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
      " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
     ]
    }
   ],
   "source": [
    "# print first 5 sentences of the corpus\n",
    "print(np.array(german_sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iT5vSgSz8C3"
   },
   "source": [
    "## Word Tokenization\n",
    "\n",
    "Word can be considered as a basic building block for NLP tasks. A combination of words make up a sentence. As in the previous section, we worked towards understanding sentence tokenization, in this section, we will focus on understanding word boundaries.\n",
    "\n",
    "We will make use of various word tokenizers available from ``nltk`` as well as ``spacy`` in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "LMbDfYiUfinQ",
    "outputId": "bf36e183-edfd-46a0-fff5-2ce62bf94533"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoL-Yo18z932",
    "outputId": "01fd3888-a007-434f-88dc-38fdfb3eec8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default tokenizer\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVmk8ZLnJG0E",
    "outputId": "20ee001f-6e93-42fb-8051-7d561a06fa92"
   },
   "outputs": [],
   "source": [
    "# utility for tokenization\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "sents = tokenize_text(sample_text)\n",
    "#np.array(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhi2_I9_JRa7",
    "outputId": "d1f2d624-c06e-4ba5-f2a8-f22d2839fabf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for sentence in sents\n",
    "                for word in sentence]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZmp-xdsKJo3"
   },
   "source": [
    "### Spacy Tokenizer\n",
    "\n",
    "``spacy`` provides easy to use interfaces to perform sentence and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8pIu_CmNJfq9"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "Hs_nMjPggJL8",
    "outputId": "06e8dfa2-da07-47d6-8ead-4fbb8a8e96c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pzIdzLjvbBXE"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-MYBs8LJRUy",
    "outputId": "9ba4b731-87e4-4a7e-e357-b704abfa1dc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_spacy = nlp(sample_text)\n",
    "type(text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DFZu8v5HspR",
    "outputId": "f0423b4e-3cb6-42cc-aeb8-73920cb8e55d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x31c268ea0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_spacy.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZtzqRkvbczl",
    "outputId": "0b099360-8cb6-4d44-cdd1-6104be9a4e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[US unveils world's most powerful supercomputer, beats China.,\n",
       " The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.,\n",
       " With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
       " Summit has 4,608 servers, which reportedly take up the size of two tennis courts.]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_spacy.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgW9AQ05JRL-",
    "outputId": "1d0a3537-7164-46ff-ba9c-ea3b63e6b3ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "        'supercomputer', ',', 'beats', 'China', '.', '', '', '', '', '',\n",
       "        '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n",
       "       ['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most',\n",
       "        'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',',\n",
       "        'beating', 'the', 'previous', 'record', '-', 'holder', 'China',\n",
       "        \"'s\", 'Sunway', 'TaihuLight', '.', '', '', '', '', ''],\n",
       "       ['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion',\n",
       "        'calculations', 'per', 'second', ',', 'it', 'is', 'over',\n",
       "        'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "        'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "        'calculations', 'per', 'second', '.'],\n",
       "       ['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly',\n",
       "        'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts',\n",
       "        '.', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(text_spacy.sents)\n",
    "\n",
    "# Determine the maximum length of sentences\n",
    "max_length = max(len(sentence) for sentence in sents)\n",
    "\n",
    "# Extract sentences as lists of token texts\n",
    "sents = [[token.text for token in sent] for sent in text_spacy.sents]\n",
    "\n",
    "# Pad sentences with empty strings to make them the same length\n",
    "padded_sents = [sentence + [\"\"] * (max_length - len(sentence)) for sentence in sents]\n",
    "\n",
    "# Convert to NumPy array\n",
    "sents = np.array(padded_sents)\n",
    "\n",
    "\n",
    "#sents = np.array(list(text_spacy.sents))\n",
    "#sents = list(text_spacy.sents)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQlWQVR8H5Te",
    "outputId": "ed787f59-a275-4676-dcfd-551540655096"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD0RXxHBggkx",
    "outputId": "c19915e7-34cd-4034-e5ad-64f97714f954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d7ERLeK8H6uL",
    "outputId": "1a366c19-ddd6-42ae-8dec-8649a39c2ad5"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "sents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2qDKo0LH-oP",
    "outputId": "2a56dab1-599d-4dde-e6df-5e5eb6554fae"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43msents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "type(sents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU_Fywt2gq2e",
    "outputId": "ed6ee459-2a73-4d58-c0e1-9f5b68a9428d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [sentence\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sents]\n",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sents]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "[sentence.text for sentence in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVudQxHTseFd",
    "outputId": "77eae63c-f468-4de0-c1e5-60af2cc58703"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.str_' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sent_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents:\n\u001b[0;32m----> 4\u001b[0m   word_list \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      5\u001b[0m   sent_words\u001b[38;5;241m.\u001b[39mappend(word_list)\n\u001b[1;32m      7\u001b[0m sent_words\n",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m sent_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents:\n\u001b[0;32m----> 4\u001b[0m   word_list \u001b[38;5;241m=\u001b[39m [\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      5\u001b[0m   sent_words\u001b[38;5;241m.\u001b[39mappend(word_list)\n\u001b[1;32m      7\u001b[0m sent_words\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.str_' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "sent_words = []\n",
    "\n",
    "for sent in sents:\n",
    "  word_list = [word.text for word in sent]\n",
    "  sent_words.append(word_list)\n",
    "\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4tVKi5_KD5S",
    "outputId": "5bc8d2fc-2adb-4a83-887b-e14f3b9169ee"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.str_' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sent_words \u001b[38;5;241m=\u001b[39m [[word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      2\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m sent\n\u001b[1;32m      3\u001b[0m                     \u001b[38;5;129;01min\u001b[39;00m sents]\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39marray(sent_words)\n",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sent_words \u001b[38;5;241m=\u001b[39m [[word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      2\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m sent\n\u001b[1;32m      3\u001b[0m                     \u001b[38;5;129;01min\u001b[39;00m sents]\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39marray(sent_words)\n",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sent_words \u001b[38;5;241m=\u001b[39m [[\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent]\n\u001b[1;32m      2\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m sent\n\u001b[1;32m      3\u001b[0m                     \u001b[38;5;129;01min\u001b[39;00m sents]\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39marray(sent_words)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.str_' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "sent_words = [[word.text for word in sent]\n",
    "                 for sent\n",
    "                    in sents]\n",
    "np.array(sent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0KIafSXzOgG",
    "outputId": "14d1ed65-127e-4331-f524-d3d5d4106014"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from text_spacy.sents to list of string-sentences\n",
    "[sent.text for sent in list(text_spacy.sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UObrTgdGKGYa",
    "outputId": "dc96ec5e-7d60-48c1-9ad8-caf8934bc6c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
       "       'the', 'previous', 'record', '-', 'holder', 'China', \"'s\",\n",
       "       'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance',\n",
       "       'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',',\n",
       "       'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway',\n",
       "       'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has',\n",
       "       '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up',\n",
       "       'the', 'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word.text for word in text_spacy]  #word tokenization\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-qnbtAZ0VAE"
   },
   "source": [
    "## Handling Non-Text Characters\n",
    "\n",
    "Natural text consists of various types of characters such as alphabets, numbers, symbols, emoticons, non-printable characters and so on. For most practical use-cases we limit ourselves to alphabets (at most numbers) and ignore other types of characters.\n",
    "\n",
    "In this section, we will focus on identification of non-text characters and how to remove them from our corpus safely.\n",
    "\n",
    "We will focus on handling following types of characters:\n",
    "- Accented Characters\n",
    "- Special Characters\n",
    "- HTML Tags & Noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzFbsU1e0put"
   },
   "source": [
    "### Accented Characters\n",
    "\n",
    "The most common accents are the acute (é), grave (è), circumflex (â, î or ô), tilde (ñ), umlaut and dieresis (ü or ï – the same symbol is used for two different purposes), and cedilla (ç). Accent marks (also referred to as diacritics or diacriticals) usually appear above a character.\n",
    "\n",
    "These characters are part of extended alphabet in languages such as French, Spanish, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5QRMw2Za0VXL"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "guR7PRqX0zEF",
    "outputId": "c0601580-5e7a-45f3-8bad-eaa874c5ad17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sómě Áccěntěd těxt'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Sómě Áccěntěd těxt'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GipXUT6yLCEy",
    "outputId": "325d679c-e111-4c46-8648-f9e6885330f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_accented_chars(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w0miO_i0ewB"
   },
   "source": [
    "### Special Characters\n",
    "\n",
    "Symbols, emoticons and characters such as ``#``, ``@`` etc. are considered special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "V-E3O1uLYw_L"
   },
   "outputs": [],
   "source": [
    "# [^a-zA-Z0-9\\s] => this will remove anything which is not a letter (eng alphabet), number or space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2rQSOYM40fGG"
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    print('Pattern used is:', pattern)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rRR5f_Vk1OqQ",
    "outputId": "09206917-6651-42e3-b4fd-dafbbe700c7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "HL0DRSHYK_Jv",
    "outputId": "892905b2-7f56-48cb-b565-be8f6caeed3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern used is: [^a-zA-Z\\s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at  What do you think  '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s, remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "dz-h-z9K1Vk8",
    "outputId": "d00fac43-403f-4058-dd57-9b9150c4c838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern used is: [^a-zA-Z0-9\\s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at 730 What do you think 9318 '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GG1e7Xh1pBJ"
   },
   "source": [
    "### HTML Tags & Noise\n",
    "\n",
    "Many times, NLP datasets are collected as part of web-scraping activities. Web-scraping involves scanning various websites to extract text from them. This process leads to content which is a mix of actual text as well as HTML tags.\n",
    "\n",
    "In this section we will extract HTML version of ** The Bible** book. We will then use ``BeautifulSoup`` to clean out HTML tags to get actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cuZWEyd_ZX8m",
    "outputId": "c0a37704-18d4-4a89-86b6-d0a6af45e03b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.gutenberg.org/cache/epub/8001/pg8001.html'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_html_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcCctZEg1pV2",
    "outputId": "958d198b-0f64-4091-f0f1-64d4cf09c730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rg.org/ebooks/8001/pg8001.html\">\n",
      "<meta property=\"og:image\" content=\"https://www.gutenberg.org/ebooks/8001/pg8001.cover.medium.jpg\">\n",
      "</head><body><section class=\"pg-boilerplate pgheader\" id=\"pg-header\" lang=\"en\"><h2 id=\"pg-header-heading\" title=\"\">The Project Gutenberg eBook of <span lang=\"en\" id=\"pg-title-no-subtitle\">The Bible, King James version, Book 1: Genesis</span></h2>\n",
      "    \n",
      "<div>This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at <a class=\"reference external\" href=\"https://www.gutenberg.org\">www.gutenberg.org</a>. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.</div>\n",
      "\n",
      "<div class=\"container\" id=\"pg-machine-header\"><p><st\n"
     ]
    }
   ],
   "source": [
    "data = requests.get(bible_html_url)\n",
    "content = data.text\n",
    "print(content[6030:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BZ5sHUtvKsZz"
   },
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])] # basically reject all script tags in HTML\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text) # remove extra blank newlines and replace with single newlines\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJiDezzJKygr",
    "outputId": "53fa14a9-f079-469c-d0c7-1a0164fe6a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g and the morning were the fifth day.\n",
      "01:001:024 And God said, Let the earth bring forth the living creature\n",
      "           after his kind, cattle, and creeping thing, and beast of the\n",
      "           earth after his kind: and it was so.\n",
      "01:001:025 And God made the beast of the earth after his kind, and cattle\n",
      "           after their kind, and every thing that creepeth upon the earth\n",
      "           after his kind: and God saw that it was good.\n",
      "01:001:026 And God said, Let us make man in our image, after our\n",
      "           likeness: and let them have dominion over the fish of the sea,\n",
      "           and over the fowl of the air, and over the cattle, and over\n",
      "           all the ea\n"
     ]
    }
   ],
   "source": [
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[4235:4900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LxLCnCFxlpf"
   },
   "source": [
    "\n",
    "That seemed to have worked like a charm!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEegW2cg0yiZ"
   },
   "source": [
    "## Text Normalization\n",
    "\n",
    "In this section, we will prepare utilities to fix different issues with textual data.\n",
    "\n",
    "- Expand Contractions\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e_cotz81RKq"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "In linguistic morphology and information retrieval, stemming is the process of reducing inflected words to their word stem, base or root form—generally a written word form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwjTfalN0U8t"
   },
   "source": [
    "#### Porter Stemmer\n",
    "\n",
    "The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "nSphmzxS1Slq"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znVyq3LLPQh_",
    "outputId": "74ba1665-765f-4e78-ad48-fb6e2b9436e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PcKf-pNkQGKy",
    "outputId": "48241a5c-5b94-4e0f-db66-eb97e8548ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wn-IMSgCQF5E",
    "outputId": "5aec2005-3fb2-4f78-9d88-af4cfd0ce1cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strang'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIgS4XM00nak"
   },
   "source": [
    "#### Lancaster Stemmer\n",
    "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nc0Mas_kQObP",
    "outputId": "2671a836-20fe-4003-f099-a290a160f631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IyYoJWvuQPYO",
    "outputId": "eaae4e13-7be2-4c5e-9dd1-c7b125c39b58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lying'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-_t_nRSoQPO1",
    "outputId": "589ea2fa-d03f-41e5-a6ac-3c300efc83e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strange'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trPCF2J61Cj5"
   },
   "source": [
    "#### Snowball Stemmer\n",
    "\n",
    "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzVzt8LHQWd4",
    "outputId": "374e3264-e062-47ab-e5f8-3a5dafb45358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DQsoQ58_QWKI",
    "outputId": "104529f0-fca7-4477-a4eb-bc492edca297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming on German words\n",
    "# autobahnen -> highway collection\n",
    "# autobahn -> single highway\n",
    "ss.stem('autobahnen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "zt9y8f0ZQWAb"
   },
   "outputs": [],
   "source": [
    "ps = nltk.porter.PorterStemmer()\n",
    "ls = nltk.stem.LancasterStemmer()\n",
    "\n",
    "def simple_stemmer(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    # alternate way\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    # text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQBpxXCb4eGh"
   },
   "source": [
    "#### Try calling the above defined function for both Lancaster and Porter stemmer separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-RK-0_Pu4rmm",
    "outputId": "210ae472-da57-4dd1-a034-e3e83d48a41c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "A-gWD0o5QVnp",
    "outputId": "79c80d27-780e-4622-ed19-884c4b4b3cd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday our crash daili and presum we are not lie'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_stemmer(s, stemmer=ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mHM0-NyF4xgi",
    "outputId": "d7c8d2ac-a43c-4db3-a952-99269341b438"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash his crash yesterday our crash dai and presum we ar not lying'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_stemmer(s, stemmer=ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh2VFpaT1TKx"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPy3H7u9e0wt",
    "outputId": "b0010360-1c9e-4dde-cf8a-52737d09861b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/laurent/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "b5J2DLu41Ues"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_y8ZYpk1VPe",
    "outputId": "9abeaba2-7491-4f74-a246-13a754faf812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "796cg1bWQtII",
    "outputId": "62fd0d39-4e6c-47e7-e3d4-85bbbd6ba089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0-Pt7hL455X",
    "outputId": "4c3862e0-d694-4e7a-f18e-e77efcfc1423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTgZtWht46AT",
    "outputId": "55aaa173-cac3-40d5-996e-84058f727a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))\n",
    "print(wnl.lemmatize('fancier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4ifadT65PZU"
   },
   "source": [
    "#### Building your own lemmatizer using nltk\n",
    "\n",
    "Define a function such that you put all the above steps together so that it does the following\n",
    "- Function name is `wordnet_lemmatize_text(...)`\n",
    "- Input is a variable text which should take in a document (bunch of words)\n",
    "- Need to tokenize the text\n",
    "- Get POS tags of tokenized text\n",
    "- Convert POS tags into wordnet (single letter) POS tags\n",
    "- use nltk's wordnet lemmatizer\n",
    "- Return lemmatized text as the output (as a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "xAFlymifbHh0"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "GKrhPLUEAkRm"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jAS_a-cu6Obm",
    "outputId": "5300051b-b63a-437d-80fa-04e525ca482c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OG4ab6tqcS78",
    "outputId": "cd338656-3ef1-478a-fae1-405e4918f991"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " 'are',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'they',\n",
       " 'are',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sleeping',\n",
       " 'lazy',\n",
       " 'dogs',\n",
       " '!']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79nbVFO0bsm6",
    "outputId": "de6bbccd-f68a-4427-a2dd-a16793a40e43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('foxes', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('quick', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('jumping', 'VBG'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sleeping', 'VBG'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dogs', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(nltk.word_tokenize(s))\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBwJ6PWillaq",
    "outputId": "8bd4357a-871e-414c-89a0-376fd152b206"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'v', 'n', 'r')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ, wordnet.VERB, wordnet.NOUN, wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jn3sgVYkdAuy",
    "outputId": "8da30cfb-c283-400b-d8f3-5f6088ed5234"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('brown', 'JJ'),\n",
       " ('foxes', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('quick', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('jumping', 'VBG'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sleeping', 'VBG'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dogs', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OMZnrUhdLw-",
    "outputId": "4414b9ac-b98d-42ea-f479-409051b17dd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'j': 'a', 'n': 'n', 'r': 'r', 'v': 'v'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dv6d54COftY8",
    "outputId": "4fca286d-50f6-44cd-b28a-7b3cdbbba779"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'JJ'[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OtCJuI3OdNi4",
    "outputId": "ed6de7dd-8bd7-41d6-874a-b894bceaf997"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map.get('JJ'[0].lower(), wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AuTTRfy4dYgn",
    "outputId": "4e619bed-b553-422a-8f4a-dd276486238f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map.get('XYZ'[0].lower(), wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qp3tA3GVcCW1",
    "outputId": "e384ec31-1de7-47be-b874-5f0b5bf45d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'n'),\n",
       " ('brown', 'a'),\n",
       " ('foxes', 'n'),\n",
       " ('are', 'v'),\n",
       " ('quick', 'a'),\n",
       " ('and', 'n'),\n",
       " ('they', 'n'),\n",
       " ('are', 'v'),\n",
       " ('jumping', 'v'),\n",
       " ('over', 'n'),\n",
       " ('the', 'n'),\n",
       " ('sleeping', 'v'),\n",
       " ('lazy', 'a'),\n",
       " ('dogs', 'n'),\n",
       " ('!', 'n')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "    for word, tag in tagged_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G20-siNJ5nFt"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def wordnet_lemmatize_text(text):\n",
    "  # tokenize text\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "\n",
    "  # pos tag tokenized text\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "  # convert raw POS tags into wordnet tags\n",
    "  tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "\n",
    "  # treat unknown tags as nouns by default\n",
    "  new_tagged_tokens = [(word, tag_map.get(tag[0].lower(),\n",
    "                                          wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "\n",
    "  lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in new_tagged_tokens)\n",
    "  return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1bERREZTmYnF",
    "outputId": "81b0450c-29a2-40ce-cfce-8a4320bfae6c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pGL2hRDR6TFy",
    "outputId": "53ff2901-8de9-4d61-f5ea-3fefa5db2dae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LaWjzXYQzFM"
   },
   "source": [
    "#### Spacy Lemmatization\n",
    "\n",
    "Out of the box implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nNZBa_U0d5DF",
    "outputId": "e511c54f-7172-4276-8cb0-5f47a9e23762"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNh6pHqqeD8z",
    "outputId": "c8af70dd-2dd2-45b5-9429-c092a5357b72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'be',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'they',\n",
       " 'be',\n",
       " 'jump',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sleep',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '!']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lemma_ for word in nlp(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GphjfKuFmf9f",
    "outputId": "938066f3-437c-4966-cbef-505969e0403d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([word.lemma_ for word in nlp(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "T8KMWjKtQwEd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# use spacy.load('en') if you have downloaded the language model en directly after install spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_\n",
    "                       if word.lemma_ != '-PRON-' else word.text\n",
    "                          for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PdTC1fcTBwPR",
    "outputId": "7c12b70a-894b-4093-82bd-3e004d657505"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qGhiUFpFQ4B_",
    "outputId": "dfb73dd4-d97c-4509-a74b-e771cc3f613f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFRvLIO7ghhN",
    "outputId": "f6714905-3685-458d-bce4-656635f42dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'be',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'they',\n",
       " 'be',\n",
       " 'jump',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sleep',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '!']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = []\n",
    "\n",
    "text = nlp(s)\n",
    "\n",
    "for word in text:\n",
    "  lemmas.append(word.lemma_)\n",
    "\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "K1ZPN2EQgrfL",
    "outputId": "f71a11d6-b32d-4ff5-9ba4-0d24ce6444b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63RQADG9RCDT"
   },
   "source": [
    "## Stopword Removal\n",
    "\n",
    "In computing, stop words are words which are filtered out before or after processing of natural language data. A stop word is a commonly used word (such as “the”, “a”, “an”,etc.) which does not convey a lot of useful information\n",
    "\n",
    "We typically remove stopwords before using text for most NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0YRm3Kncy3n",
    "outputId": "bfe07670-cf6d-4132-a19c-19abbed0513e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH2HbATve3pU"
   },
   "outputs": [],
   "source": [
    "# the fox jumps over the dog => the will be removed if is_lower_case is set to True\n",
    "# The fox jumps over the dog => the will not be removed if is_lower_case is NOT set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZCYbpMrQ9GR"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords] # keep tokens which are not in list of eng. stopwords\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVfF6BUY7AgI",
    "outputId": "fc758244-bdd3-4b1b-a7a9-3bcc0cbf8e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bo2AVwoARIm3",
    "outputId": "d8960376-3116-43bc-8d67-cf95a47bb437"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BrZP5DCp7D1c",
    "outputId": "c914c5ab-0894-4ea4-8b72-064c2704720d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s, is_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EBTFXioWfosj",
    "outputId": "49e356e6-249b-4cca-eb81-4457f95b5905"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s, is_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3-IWN2s7I2o"
   },
   "source": [
    "Remove the word 'the' and add the word 'brown' from the stop_words list and call the function with this new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHmcJUHWf6NG",
    "outputId": "b8c77686-fe54-47fa-df87-87d3e11f06a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiLGzqCG7Jt2"
   },
   "outputs": [],
   "source": [
    "stop_words.remove('the')\n",
    "stop_words.append('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DbiyGZEm7LRz",
    "outputId": "5ea39cdb-52a4-4aae-8c8e-a3319159dd0e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The foxes quick jumping the sleeping lazy dogs !'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s, is_lower_case=False, stopwords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0nU4A-_jhzwx",
    "outputId": "f345cda4-5470-451e-f654-3f31767f2dbb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfhxd1Jfh1Ag",
    "outputId": "273ac65f-14f0-496a-c8d2-3809928f3f28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'foxes', 'quick', 'jumping', 'sleeping', 'lazy', 'dogs', '!']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = []\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for word in nltk.word_tokenize(s):\n",
    "  if word.lower() not in stopwords:\n",
    "    filtered_words.append(word)\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5TD4ALNgiGGN",
    "outputId": "984d0150-0115-4ca6-f8f9-6ebf79dffb45"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9NJ07FQRXWO"
   },
   "source": [
    "## Expand Contractions\n",
    "\n",
    "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
    "\n",
    "In order to capture context better, we standardize text by expanding such contractions. ``contractions`` and ``textsearch`` enable us to do so in just a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWUKQuIhROAt",
    "outputId": "54e5cd49-201b-4409-faf7-1e78135888a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 62.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "7qyYeGwhRlPL"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bc41Ls6KRglA",
    "outputId": "68b74248-8878-49cf-c585-d887b0faaaff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'm\", 'I am'),\n",
       " (\"I'm'a\", 'I am about to'),\n",
       " (\"I'm'o\", 'I am going to'),\n",
       " (\"I've\", 'I have'),\n",
       " (\"I'll\", 'I will'),\n",
       " (\"I'll've\", 'I will have'),\n",
       " (\"I'd\", 'I would'),\n",
       " (\"I'd've\", 'I would have'),\n",
       " ('Whatcha', 'What are you'),\n",
       " (\"amn't\", 'am not')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(contractions.contractions_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0V-aX01iRcmW",
    "outputId": "900fd687-352e-49ca-ecd1-ade32fb47e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hNR1iYWlRoVc",
    "outputId": "636fb983-c1b3-4d6b-fa45-286a037d36ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
