{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iKMVCis12aq"
   },
   "source": [
    "# Topic Modeling on Research Papers\n",
    "\n",
    "We will do an interesting exercise here—build topic models on past research papers\n",
    "from the very popular NIPS conference (now known as the NeurIPS conference). The\n",
    "late professor Sam Roweis compiled an excellent collection of NIPS Conference Papers\n",
    "from Volume 1 – 12, which you can find at https://cs.nyu.edu/~roweis/data.html.\n",
    "An interesting fact is that he obtained this by massaging the OCR’d data from NIPS\n",
    "1-12, which was actually the pre-electronic submission era. Yann LeCun made the data\n",
    "available. There is an even more updated dataset available up to NIPS 17 at http://\n",
    "ai.stanford.edu/~gal/data.html. However, that dataset is in the form of a MAT file, so\n",
    "you might need to do some additional preprocessing before working on it in Python.\n",
    "\n",
    "\n",
    "# The Main Objective\n",
    "\n",
    "Considering our discussion so far, our main objective is pretty simple. Given a whole\n",
    "bunch of conference research papers, can we identify some key themes or topics from\n",
    "these papers by leveraging unsupervised learning? We do not have the liberty of labeled\n",
    "categories telling us what the major themes of every research paper are. Besides that, we\n",
    "are dealing with text data extracted using OCR (optical character recognition). Hence,\n",
    "you can expect misspelled words, words with characters missing, and so on, which\n",
    "makes our problem even more challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsTqya546U6q"
   },
   "source": [
    "# Download Data and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJaNFsqQ6KGt",
    "outputId": "1cb70c59-77aa-4a17-8e99-af61d0c35a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-21 17:57:27--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving cs.nyu.edu (cs.nyu.edu)... 216.165.22.203\n",
      "Connecting to cs.nyu.edu (cs.nyu.edu)|216.165.22.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12851423 (12M) [application/x-gzip]\n",
      "Saving to: ‘nips12raw_str602.tgz’\n",
      "\n",
      "nips12raw_str602.tg 100%[===================>]  12.26M  24.9MB/s    in 0.5s    \n",
      "\n",
      "2023-12-21 17:57:27 (24.9 MB/s) - ‘nips12raw_str602.tgz’ saved [12851423/12851423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
    "!tar -xzf nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGBr2O4hWo7W",
    "outputId": "59710d25-1683-4410-e07e-06ab9bb39d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q gensim==3.6\n",
    "!sed -i 's/collections import Mapping, defaultdict/collections.abc import Mapping;from collections import defaultdict;/' /usr/local/lib/python3.10/dist-packages/gensim/corpora/dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C36S18zo8w3b",
    "outputId": "116e7e1c-d24f-473f-c2e0-2f3fc1552d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHpf-YxR6XuW",
    "outputId": "c7e3f188-e7f4-4a44-c343-b763a75cda9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nips05', 'nips08', 'MATLAB_NOTES', 'nips06', 'RAW_DATA_NOTES', 'nips09', 'idx', 'nips11', 'nips12', 'nips10', 'README_yann', 'nips01', 'nips02', 'nips03', 'nips07', 'nips00', 'nips04', 'orig']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y20UiwtA6pAg"
   },
   "source": [
    "# Load NIPS Research Papers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3G8nUsJ6h7V",
    "outputId": "84d02410-95c7-4f2a-b1dd-22b6d629c666"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "# Read all texts into a list.\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaTh1Jll6kx9",
    "outputId": "1bd9a9bc-64bb-448b-fe3c-94da6faa6987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 \n",
      "ANALYSIS AND COMPARISON OF DIFFERENT LEARNING \n",
      "ALGORITHMS FOR PATTERN ASSOCIATION PROBLEMS \n",
      "J. Bernasconi \n",
      "Brown Boveri Research Center \n",
      "CH-5405 Baden, Switzerland \n",
      "ABSTRACT \n",
      "We investigate the behavior of different learning algorithms \n",
      "for networks of neuron-like units. As test cases we use simple pat- \n",
      "tern association problems, such as the XOR-problem and symmetry de- \n",
      "tection problems. The algorithms considered are either versions of \n",
      "the Boltzmann machine learning rule or based on the backpropagation \n",
      "of errors. We also propose and analyze a generalized delta rule for \n",
      "linear threshold units. We find that the performance of a given \n",
      "learning algorithm depends strongly on the type of units used. In \n",
      "particular, we observe that networks with 1 units quite generally \n",
      "exhibit a significantly better learning behavior than the correspon- \n",
      "ding 0,1 versions. We also demonstrate that an adaption of the \n",
      "weight-structure to the symmetries of the problem can lead to a \n",
      "drastic increase \n"
     ]
    }
   ],
   "source": [
    "print(papers[3][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNqkh2nb8qpe"
   },
   "source": [
    "# Basic Text Pre-processing\n",
    "\n",
    "We perform some basic text wrangling or preprocessing before diving into topic\n",
    "modeling. We keep things simple here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jhsbee7g8lwa",
    "outputId": "5195b2b5-1380-4a52-c1c3-cd9ebbb35eaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1740/1740 [00:36<00:00, 47.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "CPU times: user 35.8 s, sys: 417 ms, total: 36.2 s\n",
      "Wall time: 36.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tqdm\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+') # can also use nltk.word_tokenize to get word tokens for each paper\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in tqdm.tqdm(papers):\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1] # removing any single character words \\ numbers \\ symbols\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "\n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cq4EhTwP-I_U",
    "outputId": "fafdc34a-acd6-4274-fed2-7eddf3450d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neural', 'network', 'template', 'matching', 'application', 'real', 'time', 'classification', 'action', 'potential', 'real', 'neuron', 'yiu', 'fai', 'wong', 'jashojiban', 'banik', 'james', 'bower', 'division', 'engineering', 'applied', 'science', 'division', 'biology', 'california', 'institute', 'technology', 'pasadena', 'ca', 'abstract', 'much', 'experimental', 'study', 'real', 'neural', 'network', 'relies', 'proper', 'classification', 'extracellulary', 'sampled', 'neural', 'signal', 'action', 'potential', 'recorded', 'brain', 'ex', 'perimental']\n"
     ]
    }
   ],
   "source": [
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFubckun_IYp"
   },
   "source": [
    "# Build a Bi-gram Phrase Model\n",
    "\n",
    "Before feature engineering and vectorization, we want to extract some useful bi-gram\n",
    "based phrases from our research papers and remove some unnecessary terms. We\n",
    "leverage the very useful gensim.models.Phrases class for this. This capability helps us\n",
    "automatically detect common phrases from a stream of sentences, which are typically\n",
    "multi-word expressions/word n-grams.\n",
    "\n",
    "This implementation draws inspiration\n",
    "from the famous paper by Mikolov, et al., “Distributed Representations of Words and\n",
    "Phrases and their Compositionality,” which you can check out at https://arxiv.org/\n",
    "abs/1310.4546. We start by extracting and generating words and bi-grams as phrases for\n",
    "each tokenized research paper.\n",
    "\n",
    "We leverage the `min_count` parameter, which tells us that our model ignores all words and bi-grams with total\n",
    "collected count lower than 20 across the corpus (of the input paper as a list of tokenized\n",
    "sentences). We also use a `threshold` of 20, which tells us that the model accepts specific\n",
    "phrases based on this threshold value so that a phrase of words a followed by b is\n",
    "accepted if the score of the phrase is greater than the threshold of 20. This threshold is\n",
    "dependent on the scoring parameter, which helps us understand how these phrases are\n",
    "scored to understand their influence.\n",
    "Typically the default scorer is used and it’s pretty straightforward to understand.\n",
    "You can check out further details in the documentation at https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer and in the\n",
    "previously mentioned research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RWq3XtcEf21F",
    "outputId": "2ecf0b10-2429-4372-8081-25c7937061a4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__ # version 3.6.0-3.8.3 needed to run MALLET LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IX3-ZbQ-fXp",
    "outputId": "521d9aff-e437-41e6-fa09-e2e7c0b63e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neural_network', 'template_matching', 'application', 'real_time', 'classification', 'action_potential', 'real', 'neuron', 'yiu', 'fai', 'wong', 'jashojiban', 'banik', 'james', 'bower', 'division', 'engineering', 'applied', 'science', 'division_biology', 'california_institute', 'technology_pasadena', 'ca_abstract', 'much', 'experimental', 'study', 'real', 'neural_network', 'relies', 'proper', 'classification', 'extracellulary', 'sampled', 'neural', 'signal', 'action_potential', 'recorded', 'brain', 'ex', 'perimental', 'animal', 'neurophysiology', 'laboratory', 'classification', 'task', 'simplified', 'limiting', 'investigation', 'single', 'electrically']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_papers[0]][:50]) # very similar to using ngram_range=(1,2) in count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaGCzYSU-jx1",
    "outputId": "fd7b6194-0da7-44f1-85cb-aeff9f383ca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '11o'), (1, '2a'), (2, '2c'), (3, '2d'), (4, '2nl'), (5, '3a'), (6, '3b'), (7, '3c'), (8, '4a'), (9, '4b'), (10, '4c'), (11, '4d'), (12, '4n'), (13, '4n2'), (14, '4rnn')]\n",
      "Total Vocabulary Size: 78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x47ztNpD12bC"
   },
   "source": [
    "Looks like we have a lot of unique phrases in our corpus of research papers,\n",
    "based on the preceding output. Several of these terms are not very useful since they are\n",
    "specific to a paper or even a paragraph in a research paper. Hence, it is time to prune\n",
    "our vocabulary and start removing terms. Leveraging document frequency is a great way\n",
    "to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTiZ2Bxe-tNK",
    "outputId": "690a9534-314b-4654-8325-2f85d6cc73d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7756\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 60% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6) # similar to min_df and max_df in count vectorizer\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKMDWQ6W12bF"
   },
   "source": [
    "We removed all terms that occur fewer than 20 times across all documents and all\n",
    "terms that occur in more than 60% of all the documents. We are interested in finding\n",
    "different themes and topics and not recurring themes. Hence, this suits our scenario\n",
    "perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj2lB4_b_eFO"
   },
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "\n",
    "We can now perform feature engineering by leveraging a simple Bag of Words\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp_fhm91-3R-",
    "outputId": "4052434c-9d54-4b2b-843a-4dfd1ffca80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, 1), (12, 1), (19, 1), (28, 3), (38, 1), (39, 3), (40, 1), (48, 4), (49, 4), (52, 1), (58, 1), (66, 1), (70, 1), (77, 4), (83, 3), (84, 3), (85, 1), (93, 10), (94, 1), (98, 5), (112, 1), (116, 3), (118, 1), (121, 3), (122, 5), (124, 1), (127, 1), (129, 1), (130, 2), (131, 1), (134, 1), (140, 2), (142, 1), (145, 2), (150, 13), (165, 1), (170, 4), (172, 2), (175, 6), (176, 2), (182, 1), (183, 2), (195, 1), (196, 1), (198, 3), (200, 1), (206, 4), (208, 2), (217, 1), (229, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oOA652d-6sI",
    "outputId": "38b2f771-136b-4060-888a-420d3c12617d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ability', 1), ('able', 1), ('accuracy', 1), ('actual', 3), ('allowing', 1), ('although', 3), ('american_institute', 1), ('application', 4), ('applied', 4), ('around', 1), ('au', 1), ('becomes', 1), ('better', 1), ('buffer', 4), ('calculate', 3), ('calculated', 3), ('calculating', 1), ('change', 10), ('channel', 1), ('chosen', 5), ('co', 1), ('combined', 3), ('common', 1), ('compared', 3), ('comparison', 5), ('complex', 1), ('component', 1), ('computation', 1), ('computer', 2), ('computing', 1), ('conclusion', 1), ('consider', 2), ('considered', 1), ('constant', 2), ('context', 13), ('could', 1), ('current', 4), ('deal', 2), ('define', 6), ('degree', 2), ('denote', 1), ('dependent', 2), ('determine', 1), ('determined', 1), ('developed', 3), ('development', 1), ('difference', 4), ('difficulty', 2), ('discrimination', 1), ('duration', 3)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAjuxxdH--fn",
    "outputId": "800c226d-068e-4cf5-81e3-0a078ff4bb53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Total number of papers:', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUb8ZKJXJI-4"
   },
   "source": [
    "# Topic Models with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) technique is a generative probabilistic model in\n",
    "which each document is assumed to have a combination of topics similar to a probabilistic\n",
    "Latent Semantic Indexing model. In this case, the latent topics contain a Dirichlet\n",
    "prior over them. The math behind in this technique is pretty involved, so we will try to\n",
    "summarize it since going it specific details is out of the current scope.\n",
    "\n",
    "![](https://i.imgur.com/l23JAvE.png)\n",
    "\n",
    "Simplyfying the LDA model process:\n",
    "\n",
    "![](https://i.imgur.com/0BXCaUi.png)\n",
    "\n",
    "![](https://i.imgur.com/ioiUAxX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "083_UQPlJZy_",
    "outputId": "8386e6d8-5e7f-44ce-fd8e-8fb620aee6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 3.28 s, total: 4min 11s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_TOPICS = 10\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus,\n",
    "                                   id2word=dictionary, chunksize=1740,\n",
    "                                   alpha='auto', eta='auto', random_state=42,\n",
    "                                   iterations=500, num_topics=TOTAL_TOPICS,\n",
    "                                   passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IF9KX9UARO1r",
    "outputId": "88e7fade-6ae8-4fb2-9383-b8878bf6022e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.015*\"state\" + 0.008*\"dynamic\" + 0.008*\"vector\" + 0.007*\"matrix\" + 0.007*\"equation\" + 0.005*\"control\" + 0.005*\"solution\" + 0.005*\"linear\" + 0.004*\"trajectory\" + 0.004*\"nonlinear\" + 0.004*\"step\" + 0.004*\"signal\" + 0.004*\"gradient\" + 0.004*\"sequence\" + 0.003*\"convergence\" + 0.003*\"eq\" + 0.003*\"noise\" + 0.003*\"component\" + 0.003*\"attractor\" + 0.003*\"source\"\n",
      "\n",
      "Topic #2:\n",
      "0.020*\"training\" + 0.014*\"unit\" + 0.009*\"hidden_unit\" + 0.007*\"net\" + 0.007*\"prediction\" + 0.006*\"task\" + 0.006*\"trained\" + 0.005*\"training_set\" + 0.005*\"architecture\" + 0.004*\"pattern\" + 0.004*\"expert\" + 0.004*\"layer\" + 0.004*\"test\" + 0.004*\"noise\" + 0.003*\"target\" + 0.003*\"back_propagation\" + 0.003*\"vector\" + 0.003*\"generalization\" + 0.003*\"rate\" + 0.003*\"table\"\n",
      "\n",
      "Topic #3:\n",
      "0.009*\"rule\" + 0.008*\"pattern\" + 0.008*\"unit\" + 0.007*\"representation\" + 0.006*\"structure\" + 0.006*\"feature\" + 0.006*\"image\" + 0.005*\"vector\" + 0.004*\"cluster\" + 0.004*\"distance\" + 0.004*\"constraint\" + 0.003*\"transformation\" + 0.003*\"object\" + 0.003*\"node\" + 0.003*\"clustering\" + 0.003*\"training\" + 0.003*\"level\" + 0.003*\"language\" + 0.003*\"symbol\" + 0.003*\"part\"\n",
      "\n",
      "Topic #4:\n",
      "0.025*\"image\" + 0.010*\"object\" + 0.007*\"visual\" + 0.006*\"task\" + 0.006*\"target\" + 0.006*\"representation\" + 0.005*\"position\" + 0.005*\"feature\" + 0.005*\"face\" + 0.005*\"human\" + 0.005*\"subject\" + 0.005*\"location\" + 0.005*\"pixel\" + 0.004*\"view\" + 0.004*\"motion\" + 0.004*\"control\" + 0.004*\"movement\" + 0.004*\"response\" + 0.004*\"unit\" + 0.003*\"signal\"\n",
      "\n",
      "Topic #5:\n",
      "0.025*\"neuron\" + 0.010*\"cell\" + 0.008*\"signal\" + 0.007*\"response\" + 0.007*\"circuit\" + 0.007*\"spike\" + 0.006*\"current\" + 0.006*\"synaptic\" + 0.006*\"activity\" + 0.006*\"pattern\" + 0.005*\"neural\" + 0.005*\"stimulus\" + 0.005*\"voltage\" + 0.005*\"frequency\" + 0.005*\"firing\" + 0.004*\"noise\" + 0.004*\"synapsis\" + 0.004*\"channel\" + 0.004*\"threshold\" + 0.003*\"effect\"\n",
      "\n",
      "Topic #6:\n",
      "0.018*\"cell\" + 0.010*\"unit\" + 0.008*\"layer\" + 0.008*\"response\" + 0.008*\"neuron\" + 0.008*\"stimulus\" + 0.008*\"map\" + 0.007*\"activity\" + 0.007*\"receptive_field\" + 0.007*\"pattern\" + 0.007*\"visual\" + 0.006*\"motion\" + 0.006*\"orientation\" + 0.006*\"cortical\" + 0.006*\"direction\" + 0.005*\"spatial\" + 0.005*\"connection\" + 0.004*\"cortex\" + 0.004*\"eye\" + 0.004*\"region\"\n",
      "\n",
      "Topic #7:\n",
      "0.016*\"class\" + 0.016*\"classifier\" + 0.014*\"feature\" + 0.014*\"classification\" + 0.008*\"training\" + 0.008*\"pattern\" + 0.007*\"tree\" + 0.007*\"node\" + 0.007*\"probability\" + 0.005*\"sample\" + 0.003*\"vector\" + 0.003*\"test\" + 0.003*\"layer\" + 0.003*\"mlp\" + 0.003*\"distribution\" + 0.003*\"experiment\" + 0.003*\"cluster\" + 0.003*\"level\" + 0.003*\"structure\" + 0.003*\"application\"\n",
      "\n",
      "Topic #8:\n",
      "0.012*\"word\" + 0.009*\"chip\" + 0.008*\"recognition\" + 0.006*\"memory\" + 0.006*\"speech\" + 0.006*\"vector\" + 0.005*\"bit\" + 0.005*\"training\" + 0.005*\"analog\" + 0.005*\"layer\" + 0.005*\"architecture\" + 0.005*\"character\" + 0.004*\"circuit\" + 0.004*\"sequence\" + 0.004*\"unit\" + 0.004*\"processor\" + 0.004*\"hmm\" + 0.004*\"state\" + 0.004*\"node\" + 0.003*\"application\"\n",
      "\n",
      "Topic #9:\n",
      "0.008*\"distribution\" + 0.005*\"approximation\" + 0.005*\"probability\" + 0.005*\"class\" + 0.005*\"let\" + 0.005*\"bound\" + 0.005*\"linear\" + 0.005*\"vector\" + 0.004*\"variable\" + 0.004*\"training\" + 0.004*\"size\" + 0.004*\"estimate\" + 0.004*\"kernel\" + 0.004*\"sample\" + 0.004*\"theory\" + 0.004*\"theorem\" + 0.003*\"prior\" + 0.003*\"consider\" + 0.003*\"bayesian\" + 0.003*\"log\"\n",
      "\n",
      "Topic #10:\n",
      "0.025*\"state\" + 0.011*\"action\" + 0.007*\"policy\" + 0.007*\"step\" + 0.006*\"control\" + 0.006*\"reinforcement_learning\" + 0.005*\"task\" + 0.005*\"node\" + 0.004*\"environment\" + 0.004*\"probability\" + 0.004*\"optimal\" + 0.003*\"search\" + 0.003*\"reward\" + 0.003*\"goal\" + 0.003*\"rate\" + 0.003*\"current\" + 0.003*\"agent\" + 0.003*\"machine\" + 0.003*\"graph\" + 0.003*\"sequence\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jExj9SYRRyE",
    "outputId": "73b295d0-6c62-4c12-9142-13bfdb45e4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -1.0628844210789536\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtPTIGeQ12by"
   },
   "source": [
    "Topic coherence is a complex topic in its own and it can be used to measure the\n",
    "quality of topic models to some extent. Typically, a set of statements is said to be\n",
    "coherent if they support each other. Topic models are unsupervised learning based\n",
    "models that are trained on unstructured text data, making it difficult to measure the\n",
    "quality of outputs.\n",
    "\n",
    "Refer to Text Analytics with Python 2nd Edition for more detail on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azEdB08qRX4z",
    "outputId": "dd2138b0-e591-49f3-a4bf-01c8b40a2397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('training', 0.02), ('unit', 0.014), ('hidden_unit', 0.009), ('net', 0.007), ('prediction', 0.007), ('task', 0.006), ('trained', 0.006), ('training_set', 0.005), ('architecture', 0.005), ('pattern', 0.004), ('expert', 0.004), ('layer', 0.004), ('test', 0.004), ('noise', 0.004), ('target', 0.003), ('back_propagation', 0.003), ('vector', 0.003), ('generalization', 0.003), ('rate', 0.003), ('table', 0.003)]\n",
      "\n",
      "Topic #2:\n",
      "[('class', 0.016), ('classifier', 0.016), ('feature', 0.014), ('classification', 0.014), ('training', 0.008), ('pattern', 0.008), ('tree', 0.007), ('node', 0.007), ('probability', 0.007), ('sample', 0.005), ('vector', 0.003), ('test', 0.003), ('layer', 0.003), ('mlp', 0.003), ('distribution', 0.003), ('experiment', 0.003), ('cluster', 0.003), ('level', 0.003), ('structure', 0.003), ('application', 0.003)]\n",
      "\n",
      "Topic #3:\n",
      "[('neuron', 0.025), ('cell', 0.01), ('signal', 0.008), ('response', 0.007), ('circuit', 0.007), ('spike', 0.007), ('current', 0.006), ('synaptic', 0.006), ('activity', 0.006), ('pattern', 0.006), ('neural', 0.005), ('stimulus', 0.005), ('voltage', 0.005), ('frequency', 0.005), ('firing', 0.005), ('noise', 0.004), ('synapsis', 0.004), ('channel', 0.004), ('threshold', 0.004), ('effect', 0.003)]\n",
      "\n",
      "Topic #4:\n",
      "[('distribution', 0.008), ('approximation', 0.005), ('probability', 0.005), ('class', 0.005), ('let', 0.005), ('bound', 0.005), ('linear', 0.005), ('vector', 0.005), ('variable', 0.004), ('training', 0.004), ('size', 0.004), ('estimate', 0.004), ('kernel', 0.004), ('sample', 0.004), ('theory', 0.004), ('theorem', 0.004), ('prior', 0.003), ('consider', 0.003), ('bayesian', 0.003), ('log', 0.003)]\n",
      "\n",
      "Topic #5:\n",
      "[('image', 0.025), ('object', 0.01), ('visual', 0.007), ('task', 0.006), ('target', 0.006), ('representation', 0.006), ('position', 0.005), ('feature', 0.005), ('face', 0.005), ('human', 0.005), ('subject', 0.005), ('location', 0.005), ('pixel', 0.005), ('view', 0.004), ('motion', 0.004), ('control', 0.004), ('movement', 0.004), ('response', 0.004), ('unit', 0.004), ('signal', 0.003)]\n",
      "\n",
      "Topic #6:\n",
      "[('state', 0.025), ('action', 0.011), ('policy', 0.007), ('step', 0.007), ('control', 0.006), ('reinforcement_learning', 0.006), ('task', 0.005), ('node', 0.005), ('environment', 0.004), ('probability', 0.004), ('optimal', 0.004), ('search', 0.003), ('reward', 0.003), ('goal', 0.003), ('rate', 0.003), ('current', 0.003), ('agent', 0.003), ('machine', 0.003), ('graph', 0.003), ('sequence', 0.003)]\n",
      "\n",
      "Topic #7:\n",
      "[('cell', 0.018), ('unit', 0.01), ('layer', 0.008), ('response', 0.008), ('neuron', 0.008), ('stimulus', 0.008), ('map', 0.008), ('activity', 0.007), ('receptive_field', 0.007), ('pattern', 0.007), ('visual', 0.007), ('motion', 0.006), ('orientation', 0.006), ('cortical', 0.006), ('direction', 0.006), ('spatial', 0.005), ('connection', 0.005), ('cortex', 0.004), ('eye', 0.004), ('region', 0.004)]\n",
      "\n",
      "Topic #8:\n",
      "[('state', 0.015), ('dynamic', 0.008), ('vector', 0.008), ('matrix', 0.007), ('equation', 0.007), ('control', 0.005), ('solution', 0.005), ('linear', 0.005), ('trajectory', 0.004), ('nonlinear', 0.004), ('step', 0.004), ('signal', 0.004), ('gradient', 0.004), ('sequence', 0.004), ('convergence', 0.003), ('eq', 0.003), ('noise', 0.003), ('component', 0.003), ('attractor', 0.003), ('source', 0.003)]\n",
      "\n",
      "Topic #9:\n",
      "[('word', 0.012), ('chip', 0.009), ('recognition', 0.008), ('memory', 0.006), ('speech', 0.006), ('vector', 0.006), ('bit', 0.005), ('training', 0.005), ('analog', 0.005), ('layer', 0.005), ('architecture', 0.005), ('character', 0.005), ('circuit', 0.004), ('sequence', 0.004), ('unit', 0.004), ('processor', 0.004), ('hmm', 0.004), ('state', 0.004), ('node', 0.004), ('application', 0.003)]\n",
      "\n",
      "Topic #10:\n",
      "[('rule', 0.009), ('pattern', 0.008), ('unit', 0.008), ('representation', 0.007), ('structure', 0.006), ('feature', 0.006), ('image', 0.006), ('vector', 0.005), ('cluster', 0.004), ('distance', 0.004), ('constraint', 0.004), ('transformation', 0.003), ('object', 0.003), ('node', 0.003), ('clustering', 0.003), ('training', 0.003), ('level', 0.003), ('language', 0.003), ('symbol', 0.003), ('part', 0.003)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAkgJa3XRZ3m",
    "outputId": "13258d34-75ca-45db-d03b-2edf87822cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['training', 'unit', 'hidden_unit', 'net', 'prediction', 'task', 'trained', 'training_set', 'architecture', 'pattern', 'expert', 'layer', 'test', 'noise', 'target', 'back_propagation', 'vector', 'generalization', 'rate', 'table']\n",
      "\n",
      "Topic #2:\n",
      "['class', 'classifier', 'feature', 'classification', 'training', 'pattern', 'tree', 'node', 'probability', 'sample', 'vector', 'test', 'layer', 'mlp', 'distribution', 'experiment', 'cluster', 'level', 'structure', 'application']\n",
      "\n",
      "Topic #3:\n",
      "['neuron', 'cell', 'signal', 'response', 'circuit', 'spike', 'current', 'synaptic', 'activity', 'pattern', 'neural', 'stimulus', 'voltage', 'frequency', 'firing', 'noise', 'synapsis', 'channel', 'threshold', 'effect']\n",
      "\n",
      "Topic #4:\n",
      "['distribution', 'approximation', 'probability', 'class', 'let', 'bound', 'linear', 'vector', 'variable', 'training', 'size', 'estimate', 'kernel', 'sample', 'theory', 'theorem', 'prior', 'consider', 'bayesian', 'log']\n",
      "\n",
      "Topic #5:\n",
      "['image', 'object', 'visual', 'task', 'target', 'representation', 'position', 'feature', 'face', 'human', 'subject', 'location', 'pixel', 'view', 'motion', 'control', 'movement', 'response', 'unit', 'signal']\n",
      "\n",
      "Topic #6:\n",
      "['state', 'action', 'policy', 'step', 'control', 'reinforcement_learning', 'task', 'node', 'environment', 'probability', 'optimal', 'search', 'reward', 'goal', 'rate', 'current', 'agent', 'machine', 'graph', 'sequence']\n",
      "\n",
      "Topic #7:\n",
      "['cell', 'unit', 'layer', 'response', 'neuron', 'stimulus', 'map', 'activity', 'receptive_field', 'pattern', 'visual', 'motion', 'orientation', 'cortical', 'direction', 'spatial', 'connection', 'cortex', 'eye', 'region']\n",
      "\n",
      "Topic #8:\n",
      "['state', 'dynamic', 'vector', 'matrix', 'equation', 'control', 'solution', 'linear', 'trajectory', 'nonlinear', 'step', 'signal', 'gradient', 'sequence', 'convergence', 'eq', 'noise', 'component', 'attractor', 'source']\n",
      "\n",
      "Topic #9:\n",
      "['word', 'chip', 'recognition', 'memory', 'speech', 'vector', 'bit', 'training', 'analog', 'layer', 'architecture', 'character', 'circuit', 'sequence', 'unit', 'processor', 'hmm', 'state', 'node', 'application']\n",
      "\n",
      "Topic #10:\n",
      "['rule', 'pattern', 'unit', 'representation', 'structure', 'feature', 'image', 'vector', 'cluster', 'distance', 'constraint', 'transformation', 'object', 'node', 'clustering', 'training', 'level', 'language', 'symbol', 'part']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abiy8_Cg12b5"
   },
   "source": [
    "## Evaluating topic model quality\n",
    "\n",
    "We can use perplexity and coherence scores as measures to evaluate the topic\n",
    "model. Typically, lower the perplexity, the better the model. Similarly, the lower the\n",
    "UMass score and the higher the Cv score in coherence, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkOZQdZERbzG",
    "outputId": "f8649944-0c4d-4097-acc9-33b0479b0943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.4588543432552908\n",
      "Avg. Coherence Score (UMass): -1.0628844210789534\n",
      "Model Perplexity: -7.7943656886351915\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary,\n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary,\n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmuExSjK12b6"
   },
   "source": [
    "# LDA Models with MALLET\n",
    "\n",
    "The MALLET framework is a Java-based package for statistical natural language\n",
    "processing, document classification, clustering, topic modeling, information extraction,\n",
    "and other machine learning applications to text. MALLET stands for MAchine Learning\n",
    "for LanguagE Toolkit. It was developed by Andrew McCallum along with several people\n",
    "at the University of Massachusetts Amherst. The MALLET topic modeling toolkit\n",
    "contains efficient, sampling-based implementations of Latent Dirichlet Allocation,\n",
    "Pachinko Allocation, and Hierarchical LDA. To use MALLET’s capabilities, we need to\n",
    "download the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSRYjYP7VXPz",
    "outputId": "1b44c24b-edf8-41cd-fd4f-927408a5d17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-21 18:04:49--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip [following]\n",
      "--2023-12-21 18:04:49--  https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16184794 (15M) [application/zip]\n",
      "Saving to: ‘mallet-2.0.8.zip’\n",
      "\n",
      "mallet-2.0.8.zip    100%[===================>]  15.43M  35.3MB/s    in 0.4s    \n",
      "\n",
      "2023-12-21 18:04:50 (35.3 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "!unzip -q mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0J3x_IcVu8z"
   },
   "outputs": [],
   "source": [
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'\n",
    "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus,\n",
    "                                              num_topics=TOTAL_TOPICS, id2word=dictionary,\n",
    "                                              iterations=500, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQcf70_0WHDc",
    "outputId": "5f2c5315-d17e-4f91-977d-8f48ca3a6f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.5165427938470628\n",
      "Avg. Coherence Score (UMass): -1.0997840821653622\n",
      "Model Perplexity: -8.52683\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus,\n",
    "                                                             texts=norm_corpus_bigrams,\n",
    "                                                             dictionary=dictionary,\n",
    "                                                             coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus,\n",
    "                                                                texts=norm_corpus_bigrams,\n",
    "                                                                dictionary=dictionary,\n",
    "                                                                coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "# from STDOUT: <500> LL/token: -8.52683\n",
    "perplexity = -8.52683\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSL_0ICCMeYm"
   },
   "source": [
    "![](https://i.imgur.com/yAYrq59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvn6Xx9N12cA"
   },
   "source": [
    "# LDA Tuning: Finding the optimal number of topics\n",
    "\n",
    "Finding the optimal number of topics in a topic model is tough, given that it is like a\n",
    "model hyperparameter that you always have to set before training the model. We can\n",
    "use an iterative approach and build several models with differing numbers of topics and\n",
    "select the one that has the highest coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYeyeNmRWy4m"
   },
   "outputs": [],
   "source": [
    "def topic_model_coherence_generator(corpus, texts, dictionary,\n",
    "                                    start_topic_count=2, end_topic_count=10, step=1,\n",
    "                                    cpus=1):\n",
    "\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm.tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=corpus,\n",
    "                                                            num_topics=topic_nums, id2word=dictionary,\n",
    "                                                            iterations=500, workers=cpus)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model=mallet_lda_model, corpus=corpus,\n",
    "                                                                     texts=texts, dictionary=dictionary,\n",
    "                                                                     coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(mallet_lda_model)\n",
    "\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "8vgKu5RCXHrv",
    "outputId": "87658db2-dc75-4584-8902-647c4f22fc9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 20/29 [1:09:18<31:11, 207.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0ddc902c63f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_topic_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                end_topic_count=30, step=1, cpus=4)\n",
      "\u001b[0;32m<ipython-input-25-e121dc456374>\u001b[0m in \u001b[0;36mtopic_model_coherence_generator\u001b[0;34m(corpus, texts, dictionary, start_topic_count, end_topic_count, step, cpus)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                                      \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                                      coherence='c_v')\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcoherence_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_coherence_model_mallet_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcoherence_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallet_lda_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0mconfirmed_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symmetrize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36mpartial_accumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_co_occurrences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual_document\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirtual_document\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_docs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36manalyze_text\u001b[0;34m(self, window, doc_num)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slide_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uniq_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# to exclude none token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n",
    "                                                               dictionary=dictionary, start_topic_count=2,\n",
    "                                                               end_topic_count=30, step=1, cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeQIjHsOXPvY"
   },
   "outputs": [],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBSRO_VqYjif"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(2, 31, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7PlAhgx12cI"
   },
   "source": [
    "We choose the optimal number of topics as 15, based on our intuition. We can retrieve the best model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtPwVFdrYlxO"
   },
   "outputs": [],
   "source": [
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 15].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krcnLTbWNCBY"
   },
   "outputs": [],
   "source": [
    "best_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus,\n",
    "                                              num_topics=15, id2word=dictionary,\n",
    "                                              iterations=500, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2O-Uyy5NsLW"
   },
   "outputs": [],
   "source": [
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKh2vVkMNvwT"
   },
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3))\n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)]\n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxtpl5Co12cL"
   },
   "source": [
    "# Viewing LDA Model topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3yf_xhqN0tH"
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame([[term for term, wt in topic]\n",
    "                              for topic in topics],\n",
    "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvyOKlu4p7UG"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])\n",
    "                              for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_lda_model.num_topics+1)]\n",
    "                         )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrveLQ5e12cP"
   },
   "source": [
    "# Interpreting Topic Model Results\n",
    "\n",
    "An interesting point to remember is, given a corpus of documents (in the form of\n",
    "features, e.g., Bag of Words) and a trained topic model, you can predict the distribution of\n",
    "topics in each document (research paper in this case).\n",
    "\n",
    "We can now get the most dominant topic per research paper with some intelligent\n",
    "sorting and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYz9Pf4UN_VJ"
   },
   "outputs": [],
   "source": [
    "print(bow_corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojE7cAX7ORuS"
   },
   "outputs": [],
   "source": [
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[0][:30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXv0NRCXum-I"
   },
   "outputs": [],
   "source": [
    "tm_results = best_lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0pWhZX1OeHf"
   },
   "outputs": [],
   "source": [
    "tm_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjEHWhVGuuG5"
   },
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0]\n",
    "                     for topics in tm_results]\n",
    "corpus_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFmddPmQuyXS"
   },
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = [paper[:500] for paper in papers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_nNj9_t12cb"
   },
   "source": [
    "# Dominant Topics in Specific Research Papers\n",
    "\n",
    "Another interesting perspective is to select specific papers, view the most dominant topic\n",
    "in each of those papers, and see if that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN45exiiu5sx"
   },
   "outputs": [],
   "source": [
    "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'],\n",
    "                                                                                         ascending=False)\n",
    "                                                                             .iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqQRtvzsRSrs"
   },
   "source": [
    "# Inference on existing papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0QF6K53v4zN"
   },
   "outputs": [],
   "source": [
    "sample_paper_patterns = ['Feudal Reinforcement Learning \\nPeter', 'Illumination-Invariant Face Recognition with a', 'Improved Hidden Markov Model Speech Recognition']\n",
    "sample_paper_idxs = [idx for pattern in sample_paper_patterns\n",
    "                            for idx, content in enumerate(papers)\n",
    "                                if pattern in content]\n",
    "sample_paper_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7JrXUbsw0bt"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document']\n",
    "                 .isin(sample_paper_idxs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S84f-T7RVED"
   },
   "source": [
    "# Topic Inference on New Papers (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50rVOoraO48k"
   },
   "outputs": [],
   "source": [
    "new_paper = \"\"\"\n",
    "Unsupervised Translation of Programming Languages\n",
    "Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample\n",
    "A transcompiler, also known as source-to-source translator, is a system that converts source code\n",
    "from a high-level programming language (such as C++ or Python) to another.\n",
    "Transcompilers are primarily used for interoperability, and to port codebases\n",
    "written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one.\n",
    "They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree.\n",
    "Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions,\n",
    "and require manual modifications in order to work properly.\n",
    "The overall translation process is timeconsuming and requires expertise in both the source and target languages,\n",
    "making code-translation projects expensive.\n",
    "Although neural models significantly outperform their rule-based counterparts in the context of natural language translation,\n",
    "their applications to transcompilation have been limited due to the scarcity of parallel data in this domain.\n",
    "In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully\n",
    "unsupervised neural transcompiler. We train our model on source code from open source GitHub projects,\n",
    "and show that it can translate functions between C++, Java, and Python with high accuracy.\n",
    "Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages,\n",
    "and can easily be generalized to other programming languages. We also build and release a test set composed of 852\n",
    "parallel functions, along with unit tests to check the correctness of translations.\n",
    "We show that our model outperforms rule-based commercial baselines by a significant margin.\n",
    "\"\"\"\n",
    "\n",
    "new_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOY_6U8zRYkb"
   },
   "source": [
    "## Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0I7hWMGPQ_q"
   },
   "outputs": [],
   "source": [
    "preprocessed_papers = normalize_corpus([new_paper])\n",
    "print(preprocessed_papers[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkf7nKhURaYX"
   },
   "source": [
    "## Generate Influential Bi-grams if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUzIztYHPYeU"
   },
   "outputs": [],
   "source": [
    "bigrams_corpus = [bigram_model[doc] for doc in preprocessed_papers]\n",
    "print(bigrams_corpus[0][90:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NWsbx_cRep7"
   },
   "source": [
    "## Generate BOW Vectors from Training Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypm5UXrZPtC6"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in bigrams_corpus]\n",
    "print(bow_corpus[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqXuFNw3RiLg"
   },
   "source": [
    "## Use trained topic model to predict topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWSAMumoQGTB"
   },
   "outputs": [],
   "source": [
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[0][:30]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsQlCfP2Rmpp"
   },
   "source": [
    "## Show most relevant topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ly1ZQkzQLhW"
   },
   "outputs": [],
   "source": [
    "predicted_topics = best_lda_model[bow_corpus][0]\n",
    "predicted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTYgVVlRQUU4"
   },
   "outputs": [],
   "source": [
    "top_topic = max(predicted_topics, key=lambda x: x[1])\n",
    "top_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MypZEfwQbrl"
   },
   "outputs": [],
   "source": [
    "top_topic_idx = top_topic[0]\n",
    "top_topic_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI3XLFfkQrPo"
   },
   "outputs": [],
   "source": [
    "topics_df.iloc[[top_topic_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1VYw5guQ01h"
   },
   "outputs": [],
   "source": [
    "print(new_paper)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
