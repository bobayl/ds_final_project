{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf41728-8de4-4b0b-bbc6-13ba6a302680",
   "metadata": {},
   "source": [
    "# Web-Scraping www.avherald.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a898e3ba-998e-4af0-9d2d-6ab4a15c5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd98ed3-463d-421a-85e1-b671dbd6a8cb",
   "metadata": {},
   "source": [
    "## Define scraping functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd089b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(url):\n",
    "    \"\"\" Returns the response for the URL\"\"\"\n",
    "    ### Load the first page:\n",
    "    page = requests.get(url, headers=headers, timeout=5)\n",
    "\n",
    "    return page\n",
    "\n",
    "def get_next_page_href(page):\n",
    "    \"\"\"On a given webpage, finds and returns the URL for the 'next' page\"\"\"\n",
    "\n",
    "    # Convert page to soup object\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Find the URL for the next page\n",
    "    # Find the <img> tag with the specified src attribute\n",
    "    next_tag = soup.find('img', {'src': '/images/next.jpg'})\n",
    "\n",
    "    if next_tag:\n",
    "        # Find the parent <a> tag\n",
    "        parent_a_tag = next_tag.find_parent('a')\n",
    "        \n",
    "        if parent_a_tag:\n",
    "            # Get the href attribute of the parent <a> tag\n",
    "            next_page_url = parent_a_tag.get('href')\n",
    "        else:\n",
    "            next_page_url = None\n",
    "            print(\"Parent <a> tag not found.\")\n",
    "    else:\n",
    "        next_page_url = None\n",
    "        print(\"Image with the specified src attribute not found.\")\n",
    "\n",
    "    return next_page_url\n",
    "\n",
    "def get_article_titles_and_hrefs(page):\n",
    "    \"\"\" For a given page, returns all article hrefs as a list\"\"\"\n",
    "    # Convert to soup object\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Locate all the links and store them in the list 'hrefs'\n",
    "\n",
    "    # Find all <span> elements with the class 'headline_avherald'\n",
    "    spans = soup.find_all('span', {'class': 'headline_avherald'})\n",
    "\n",
    "    # Print or process the found spans\n",
    "    article_hrefs = []\n",
    "    article_titles = []\n",
    "    for span in spans:\n",
    "        # Get the title\n",
    "        article_title = span.get_text()\n",
    "        article_titles.append(article_title)\n",
    "        # Get the parent <a> tag\n",
    "        parent_a_tag = span.find_parent('a')\n",
    "        if parent_a_tag:\n",
    "            # Get the href\n",
    "            href = parent_a_tag.get('href')\n",
    "            article_hrefs.append(href)\n",
    "\n",
    "    return article_titles, article_hrefs\n",
    "\n",
    "def get_all_titles_and_hrefs(url):\n",
    "    # Create empty lists for titles and hrefs:\n",
    "    article_titles = []\n",
    "    article_hrefs = []\n",
    "\n",
    "    # Load home page\n",
    "    page = load_page(url)\n",
    "\n",
    "    # Get article titles and hrefs from the first page (2 separate lists)\n",
    "    titles, articles = get_article_titles_and_hrefs(page)\n",
    "    article_titles += titles\n",
    "    article_hrefs += articles\n",
    "    # Get next page URL\n",
    "    next_page_href = get_next_page_href(page)\n",
    "\n",
    "    # While next_page_URL:\n",
    "    counter = 0\n",
    "    #while counter < number_of_pages:\n",
    "    while next_page_href:\n",
    "        print(f'Page: {counter}', end='\\r', flush=True)\n",
    "        url = \"https://avherald.com\" + next_page_href\n",
    "        page = load_page(url)\n",
    "\n",
    "        titles, articles = get_article_titles_and_hrefs(page)\n",
    "        article_titles += titles\n",
    "        article_hrefs += articles\n",
    "        \n",
    "        next_page_href = get_next_page_href(page)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    title_dict = dict(zip(article_titles, article_hrefs))\n",
    "    \n",
    "    return title_dict\n",
    "\n",
    "def write_titles(title_dict):\n",
    "    # Specify the file path\n",
    "    file_path = 'titles.csv'\n",
    "\n",
    "    # Open the file in write mode ('w', newline='') to ensure newline character handling\n",
    "    with open(file_path, 'w', newline='') as f:\n",
    "        # Define the fieldnames for the CSV header\n",
    "        fieldnames = ['title', 'href']\n",
    "\n",
    "        # Create a CSV writer object\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Iterate over dictionary items and write them as rows\n",
    "        for key, value in title_dict.items():\n",
    "            writer.writerow({'title': key, 'href': value})\n",
    "\n",
    "def get_article(page):\n",
    "    \"\"\" For a given article page, returns the article text and the timestamp (including author)\"\"\"\n",
    "    # Convert to soup object\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Locate all the links and store them in the list 'hrefs'\n",
    "\n",
    "    # Find the <span> element with the class 'sitetext'\n",
    "    texts = soup.find_all('span', {'class': 'sitetext'})\n",
    "    if len(texts) > 3:\n",
    "        article_text = texts[3].get_text()\n",
    "    else:\n",
    "        article_text = \"xxx\"\n",
    "\n",
    "    time_author = soup.find('span', {'class': 'time_avherald'})\n",
    "    if time_author is not None:\n",
    "        time_author = time_author.get_text()\n",
    "    else:\n",
    "        time_author = \"yyy\"\n",
    "\n",
    "    return article_text, time_author\n",
    "\n",
    "def get_comments(page):\n",
    "    \"\"\" For a given article page, returns the title and the comments\"\"\"\n",
    "        # Convert to soup object\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Find the <span> element with the class 'headline_article', which is the article headline containing the event date\n",
    "    headline = soup.find('span', {'class': 'headline_article'})\n",
    "    if headline:\n",
    "        headline_text = headline.get_text()\n",
    "    else:\n",
    "        headline_text = None\n",
    "\n",
    "    # Finde comments and comment authors\n",
    "    comment_authors = soup.find_all('span', {'class': 'time_avherald'})\n",
    "    comments = soup.find_all('span', {'class': 'sitecomment'})\n",
    "\n",
    "    comment_authors_texts = []\n",
    "    comments_texts = []\n",
    "\n",
    "    for comment_author in comment_authors[1:]: # The class 'time_avherald' is also used just below the headline, so we omit that one\n",
    "        comment_authors_texts.append(comment_author.get_text())\n",
    "    for comment in comments[:-1]:\n",
    "        comments_texts.append(comment.get_text())\n",
    "\n",
    "    return headline_text, comment_authors_texts, comments_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2bbfc",
   "metadata": {},
   "source": [
    "## Scrape the article hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image with the specified src attribute not found.\n",
      "28963\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# # Entry\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
    "# URL = \"https://avherald.com\"\n",
    "\n",
    "# # Scrape titles and hrefs to articles\n",
    "# titles = get_all_titles_and_hrefs(URL)\n",
    "# print(len(titles))\n",
    "# write_titles(titles)\n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9f0ca",
   "metadata": {},
   "source": [
    "## Scrape the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8576a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape articles and write them to a file\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
    "# URL = \"https://avherald.com\"\n",
    "# LAST_ENTRY = 2810\n",
    "\n",
    "# def scrape_articles(title_file):\n",
    "    # \"\"\" From the titles.csv, line by line opens the page, gets the text and the timestamp and appends it to 'articles.csv' \"\"\"\n",
    "\n",
    "    # # Set a counter to keep track\n",
    "    # counter = 0\n",
    "\n",
    "    # # Open title.csv\n",
    "    # input_file_path = title_file\n",
    "    # output_file_path = \"articles.csv\"\n",
    "    # fieldnames = ['title', 'href', 'text', 'time_author']\n",
    "\n",
    "    # # Open the CSV file\n",
    "    # with open(input_file_path, mode='r', newline='') as input_file:\n",
    "    #     # Open output file for writing ('w' mode)\n",
    "    #     with open(output_file_path, 'a', newline='', encoding='utf-8') as output_file:\n",
    "    #         reader = csv.DictReader(input_file)\n",
    "    #         writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    #         # # Write header row\n",
    "    #         # writer.writeheader()\n",
    "        \n",
    "    #         # Iterate over each row in the CSV file\n",
    "    #         for row in reader:\n",
    "    #             # Increment counter\n",
    "    #             counter += 1\n",
    "    #             print(f'article: {counter}', end='\\r', flush=True)\n",
    "\n",
    "    #             # # Break for testing:\n",
    "    #             # if counter == 800:\n",
    "    #             #     break\n",
    "\n",
    "    #             if counter > LAST_ENTRY:\n",
    "    #                 # Access the 'href' column in each row\n",
    "    #                 href_value = str(row[\"href\"])\n",
    "    #                 title = row[\"title\"]\n",
    "\n",
    "    #                 # Create the page url\n",
    "    #                 url = URL + href_value\n",
    "\n",
    "    #                 # Load page:\n",
    "    #                 page = load_page(url)\n",
    "    #                 text, time_author = get_article(page)\n",
    "\n",
    "    #                 # Write to output file title, href, text, time\n",
    "    #                 row = {\"title\": title, \"href\": href_value, \"text\": text, \"time_author\": time_author}\n",
    "    #                 writer.writerow(row)\n",
    "\n",
    "    #                 # Add a time delay of 1s:\n",
    "    #                 time.sleep(1)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "    # print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0de3223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!le: 28963\n"
     ]
    }
   ],
   "source": [
    "#scrape_articles(\"titles.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3230aa0",
   "metadata": {},
   "source": [
    "## Scrape Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9972e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape headline, comments with authors and write them to a file\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
    "URL = \"https://avherald.com\"\n",
    "LAST_ENTRY = 0\n",
    "\n",
    "def scrape_comments(title_file):\n",
    "    \"\"\" From the titles.csv, line by line opens the page, gets the text and the timestamp and appends it to 'articles.csv' \"\"\"\n",
    "\n",
    "    # Set a counter to keep track\n",
    "    counter = 1\n",
    "\n",
    "    # Open title.csv\n",
    "    input_file_path = title_file\n",
    "    output_file_path = \"comments.csv\"\n",
    "    fieldnames = ['headline', 'href', 'comment_authors', 'comments']\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(input_file_path, mode='r', newline='') as input_file:\n",
    "        # Open output file for writing ('w' mode)\n",
    "        with open(output_file_path, 'a', newline='', encoding='utf-8') as output_file:\n",
    "            reader = csv.DictReader(input_file)\n",
    "            writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "            # # Write header row\n",
    "            # writer.writeheader()\n",
    "        \n",
    "            # Iterate over each row in the CSV file\n",
    "            for row in reader:\n",
    "                # Increment counter\n",
    "                counter += 1\n",
    "                print(f'article: {counter}', end='\\r', flush=True)\n",
    "\n",
    "                # # Break for testing:\n",
    "                # if counter == 10:\n",
    "                #     break\n",
    "\n",
    "                if counter > LAST_ENTRY:\n",
    "                    # Access the 'href' column in each row\n",
    "                    href_value = str(row[\"href\"])\n",
    "\n",
    "                    # Create the page url\n",
    "                    url = URL + href_value\n",
    "\n",
    "                    # Load page:\n",
    "                    page = load_page(url)\n",
    "                    headline, comment_authors, comments = get_comments(page)\n",
    "\n",
    "                    # Write to output file title, href, text, time\n",
    "                    row = {\"headline\": headline, \"href\": href_value, \"comment_authors\": comment_authors, \"comments\": comments}\n",
    "                    writer.writerow(row)\n",
    "\n",
    "                    # Add a time delay of 1s:\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "48e6da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: 12\r"
     ]
    }
   ],
   "source": [
    "#scrape_comments(\"titles.csv\")\n",
    "scrape_comments(\"missing_href.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794fee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write an update function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9ee85-bfca-4bd0-9eef-4c25609987dd",
   "metadata": {},
   "source": [
    "## Create the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d2747-8c70-4ddf-94a0-272c3bda8234",
   "metadata": {},
   "source": [
    "# Analysis of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbaa89-bb4a-4b03-bf11-85569713b000",
   "metadata": {},
   "source": [
    "## 1. How many jobs are shared between these categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b907c2b-1a62-436a-b171-d4b382562d9c",
   "metadata": {},
   "source": [
    "## 2. How much do the keywords “Data Analyst” and “Big Data Analyst” overlap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f1956-b69a-4e1e-9f8a-dc56352c74f3",
   "metadata": {},
   "source": [
    "## 3. Are there some companies doing more hires than average?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e8999-6c5c-4e7d-b94e-1fadde16638a",
   "metadata": {},
   "source": [
    "## 4. How many jobs are there in different cantons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddda6f-ff75-4bad-be93-c0a510168496",
   "metadata": {},
   "source": [
    "## 5. Is “machine learning” keyword more often in data scientist or data analyst jobs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce07b9-e2de-45ce-b47d-92142cba9a48",
   "metadata": {},
   "source": [
    "## 6. What is the distribution of most common keywords between and across categories?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
